{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24000b07-2991-4ee4-8cb1-10ce6e2e5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from src.data_collection import get_data\n",
    "from src.models import HateDataset, DistilBERTMultiClass, get_distil_hyperparams\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb52a14-08fa-4189-ae85-634e0ac3a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-7cb9b0b8e4d0e1dd\n",
      "Reusing dataset parquet (C:\\Users\\UTKARSH\\.cache\\huggingface\\datasets\\parquet\\ucberkeley-dlab--measuring-hate-speech-7cb9b0b8e4d0e1dd\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571f78f777e044e9a843403ecf42cc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "hate_speech_ucb = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222d2c0d-43ac-47dc-a2ec-b4ab7d9d9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = get_distil_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b9855c-f96f-4f87-97ac-55ca0d44ceb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = HYPERPARAMS[\"DEVICE\"]\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14a1e8b-1c67-4022-904f-42d44ba99c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = HYPERPARAMS[\"MAX_LEN\"]\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = HYPERPARAMS[\"LEARNING_RATE\"]\n",
    "TOKENIZER = HYPERPARAMS[\"TOKENIZER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e54fc0d-33b8-4f11-91dc-49eb74d98993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39565 entries, 0 to 39564\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        39565 non-null  object\n",
      " 1   hatespeech  39565 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 618.3+ KB\n"
     ]
    }
   ],
   "source": [
    "hate_speech_ucb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d6e35e-0924-487e-9cb7-bc12d57c9846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26608\n",
       "1    12957\n",
       "Name: hatespeech, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_ucb[\"hatespeech\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51955c44-f215-4c16-9eb9-c97292c757fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purpose\n",
    "# hate_speech_ucb = hate_speech_ucb[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0617aa9-d2e3-4956-a51f-5815c09b0e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hatespeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! thank u! im transmasc and generally present masc i Maybe look a bit gay so i was wondering about how that would go. im sorry u were heckled but its good to hear things are better there now. tysm my favorite lesbian :)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!Go fuck yourself faggot!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!flair [I love women and minorities]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!flair [death to all niggers and gays]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" 'convoluted' genealogy of Jesus\"; was that comment really necessary? I am sure that you have regular Christian viewers--like myself--who might not have appreciated that very much.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39560</th>\n",
       "      <td>ðŸ¤§  The limousine liberal Jews and WASPs who want to assuage their white guilt.. SJW maladjusted gays and nigs so they can pester and bother me; bullshit wack monotheist Judeo-Christianity values that's I'm suck when I leave Chinatown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39561</th>\n",
       "      <td>ðŸ¤¬CONGRATULATIONS #MARYLAND #BALTIMORE LIBERALS &amp; OBAMA... YOU REACHED A NEW LOW If I didn't know better, I would have thought this was Europe. Once again, Democrat Mayor Police Video Shows Whites Being Targeted During Memorial Day Chaos in Baltimore URL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39562</th>\n",
       "      <td>ðŸ¥³ another bitch will when another bitch can't</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39563</th>\n",
       "      <td>ðŸ¥´ðŸ¥´ next time stay your ass in the car before our \"beaner\" asses beat yah URL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39564</th>\n",
       "      <td>ðŸ§¬ðŸ§¬ðŸ§¬ *IM SMARTER THAN SSSNIPERWOLF.I KNOW EVERYTHING ABOUT RACE AND COLOR SCIENCE. SSSNIPERWOLF WANTS SWEDES TO FREEZE AND BLACKS TO BURN. My videos are the only TRUTH ABOUT RACE WHITE AND BLACK. IM WHITE AND I KNOW EVERYTHING ABOUT BLACK PEOPLE* HI SNIPERWOLF I LIVE IN GLENDALE TOO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39565 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                             text  \\\n",
       "0                                                                     ! thank u! im transmasc and generally present masc i Maybe look a bit gay so i was wondering about how that would go. im sorry u were heckled but its good to hear things are better there now. tysm my favorite lesbian :)   \n",
       "1                                                                                                                                                                                                                                                                       !Go fuck yourself faggot!   \n",
       "2                                                                                                                                                                                                                                                            !flair [I love women and minorities]   \n",
       "3                                                                                                                                                                                                                                                          !flair [death to all niggers and gays]   \n",
       "4                                                                                                           \" 'convoluted' genealogy of Jesus\"; was that comment really necessary? I am sure that you have regular Christian viewers--like myself--who might not have appreciated that very much.   \n",
       "...                                                                                                                                                                                                                                                                                           ...   \n",
       "39560                                                   ðŸ¤§  The limousine liberal Jews and WASPs who want to assuage their white guilt.. SJW maladjusted gays and nigs so they can pester and bother me; bullshit wack monotheist Judeo-Christianity values that's I'm suck when I leave Chinatown   \n",
       "39561                               ðŸ¤¬CONGRATULATIONS #MARYLAND #BALTIMORE LIBERALS & OBAMA... YOU REACHED A NEW LOW If I didn't know better, I would have thought this was Europe. Once again, Democrat Mayor Police Video Shows Whites Being Targeted During Memorial Day Chaos in Baltimore URL   \n",
       "39562                                                                                                                                                                                                                                               ðŸ¥³ another bitch will when another bitch can't   \n",
       "39563                                                                                                                                                                                                                ðŸ¥´ðŸ¥´ next time stay your ass in the car before our \"beaner\" asses beat yah URL   \n",
       "39564  ðŸ§¬ðŸ§¬ðŸ§¬ *IM SMARTER THAN SSSNIPERWOLF.I KNOW EVERYTHING ABOUT RACE AND COLOR SCIENCE. SSSNIPERWOLF WANTS SWEDES TO FREEZE AND BLACKS TO BURN. My videos are the only TRUTH ABOUT RACE WHITE AND BLACK. IM WHITE AND I KNOW EVERYTHING ABOUT BLACK PEOPLE* HI SNIPERWOLF I LIVE IN GLENDALE TOO   \n",
       "\n",
       "       hatespeech  \n",
       "0               0  \n",
       "1               1  \n",
       "2               0  \n",
       "3               1  \n",
       "4               0  \n",
       "...           ...  \n",
       "39560           1  \n",
       "39561           0  \n",
       "39562           1  \n",
       "39563           1  \n",
       "39564           0  \n",
       "\n",
       "[39565 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7e0254-57a2-431b-926d-e00baa0a20a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (39565, 2)\n",
      "TRAIN Dataset: (31652, 2)\n",
      "VAL Dataset: (3957, 3)\n",
      "TEST Dataset: (3956, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "\n",
    "train_data = hate_speech_ucb.sample(frac=train_size, random_state=210)\n",
    "test_data = hate_speech_ucb.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = test_data.sample(frac=val_size / (1 - train_size), random_state=220).reset_index()\n",
    "test_data = test_data.drop(val_data.index).reset_index(drop=True)\n",
    "\n",
    "print(f\"FULL Dataset: {hate_speech_ucb.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_data.shape}\")\n",
    "print(f\"VAL Dataset: {val_data.shape}\")\n",
    "print(f\"TEST Dataset: {test_data.shape}\")\n",
    "\n",
    "training_set = HateDataset(train_data, TOKENIZER, MAX_LEN)\n",
    "validation_set = HateDataset(val_data, TOKENIZER, MAX_LEN)\n",
    "testing_set = HateDataset(test_data, TOKENIZER, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33fd0ff6-f04d-4cac-8a49-af7089ae19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = HYPERPARAMS[\"TRAIN_PARAMS\"]\n",
    "val_params = HYPERPARAMS[\"DEV_PARAMS\"]\n",
    "test_params = HYPERPARAMS[\"TEST_PARAMS\"]\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validation_loader = DataLoader(validation_set, **val_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d28ec9-faf4-43ff-ad17-e486f6933f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = hate_speech_ucb[\"hatespeech\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce980064-cc77-4f8a-83c1-bd000bcef26d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBERTMultiClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBERTMultiClass(n_classes=N_CLASSES)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e17ea5e-1dec-43d2-bd11-5a7d80fad6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "132df9ce-366f-4aed-aaf6-519f21c3018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d69af06e-1a83-40e8-88c6-8a2a8d6552ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _, data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype=torch.float)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _ % 1000 == 0:\n",
    "            print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9105846-c745-4002-887d-7120812fce14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\UTKARSH\\miniconda3\\envs\\env_deep\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:02,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.7049654722213745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [04:25,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.3837555944919586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [08:46,  3.76it/s]\n",
      "1it [00:00,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.565147876739502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [04:28,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.3691707253456116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [08:52,  3.72it/s]\n",
      "1it [00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.24640335142612457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [04:29,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.22634290158748627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [08:53,  3.71it/s]\n",
      "1it [00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.24425974488258362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [04:28,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.44521617889404297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [08:49,  3.74it/s]\n",
      "1it [00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.08722229301929474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [04:25,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.08093215525150299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [08:46,  3.76it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a62ecd-d64b-46a6-8913-4c9f14df2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loader):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(loader, 0)):\n",
    "            ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = data[\"targets\"].to(device, dtype=torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e08ffa6-b41d-44c1-91fa-c99d961d86a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3957it [00:48, 81.98it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(model, validation_loader)\n",
    "\n",
    "final_outputs = np.argmax(outputs, axis=1)\n",
    "targets = np.argmax(targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36f85c86-374e-430e-ba42-e99448e0221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3030 / 3957 correct\n"
     ]
    }
   ],
   "source": [
    "print(f\"Got {sum(final_outputs == targets)} / {len(final_outputs)} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18486d67-6d06-4670-b895-4680a8ebd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 score:\t\t0.766\n",
      "Macro F1 score:\t\t0.716\n",
      "Weighted F1 score:\t0.757\n"
     ]
    }
   ],
   "source": [
    "micro_f1 = f1_score(targets, final_outputs, average=\"micro\")\n",
    "macro_f1 = f1_score(targets, final_outputs, average=\"macro\")\n",
    "weighted_f1 = f1_score(targets, final_outputs, average=\"weighted\")\n",
    "\n",
    "print(f\"Micro F1 score:\\t\\t{round(micro_f1, 3)}\")\n",
    "print(f\"Macro F1 score:\\t\\t{round(macro_f1, 3)}\")\n",
    "print(f\"Weighted F1 score:\\t{round(weighted_f1, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ffe1802-2337-4911-a317-5f6301975dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83      2665\n",
      "           1       0.68      0.53      0.60      1292\n",
      "\n",
      "    accuracy                           0.77      3957\n",
      "   macro avg       0.74      0.71      0.72      3957\n",
      "weighted avg       0.76      0.77      0.76      3957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets, final_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5e411ac-7ed9-407c-a3d6-144e99320f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "output_model_file = HYPERPARAMS[\"MODEL_PATH\"]\n",
    "output_vocab_file = HYPERPARAMS[\"VOCAB_PATH\"]\n",
    "\n",
    "torch.save(model.state_dict(), output_model_file)\n",
    "TOKENIZER.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_deep]",
   "language": "python",
   "name": "conda-env-env_deep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
